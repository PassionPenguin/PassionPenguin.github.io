<!doctype html>
<html lang="en" article>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>谷歌 DeepMind 发布 NFNet：高效的深度网络 - Hoarfroster</title>
    <link rel="stylesheet" href="/assets/styles/post.css">
</head>
<body>
<div class="container post markdown-body"><blockquote>
<ul>
<li>原文地址：<a href="https://www.infoq.com/news/2021/03/deepmind-NFNets-deep-learning/">Google DeepMind’s NFNets Offers Deep Learning Efficiency</a></li>
<li>原文作者：<a href="https://www.infoq.com/profile/Bruno-Santos/">Bruno-Santos</a></li>
<li>译文出自：<a href="https://github.com/xitu/gold-miner">掘金翻译计划</a></li>
<li>本文永久链接：<a href="https://github.com/xitu/gold-miner/blob/master/article/2021/deepmind-NFNets-deep-learning.md">https://github.com/xitu/gold-miner/blob/master/article/2021/deepmind-NFNets-deep-learning.md</a></li>
<li>译者：<a href="https://github.com/chzh9311">chzh9311</a></li>
<li>校对者：<a href="https://github.com/PassionPenguin">PassionPenguin</a>、<a href="https://github.com/shentaowang">Shentaowang</a></li>
</ul>
</blockquote>
<h1>谷歌 DeepMind 发布 NFNet：高效的深度网络</h1>
<p>谷歌旗下的 AI 公司 DeepMind 最近发布了 NFNet，一个不需要标准化的 ResNet 图像分类模型。相比于当前表现最佳的 <a href="https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html">EfficientNet</a>，这一模型的<a href="https://arxiv.org/abs/2102.06171">训练速度快了 8.7 倍</a>。</p>
<p>据谷歌 DeepMind 的研究员所说（参考下面的图表）：</p>
<blockquote>
<p>NFNet-F1 模型达到了和 EfficientNet-B7 相近的精度，但训练速度却比后者快了 8.7 倍。而且我们最大的模型在不利用额外数据的情况下 top-1 精度达到了 86.5%，树立了新的前沿标准。</p>
</blockquote>
<p><img src="../images/deepmind-NFNets-deep-learning.md-11figure-1-1616684541530.jpg" alt=""></p>
<p>对于大规模的图像识别任务，通常神经网络会使用一种叫<a href="http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture07.pdf">批标准化（batch normalization）</a>的技术来让模型训练更高效。此外，这种技术也让神经网络泛化性能更好，换言之，它正则化的作用。</p>
<p>然而，批标准化存在一些缺点，比如训练和预测的时候表现不相符。而且由于<a href="https://www.wikiwand.com/en/Backpropagation">反向传播</a>（backpropagation，神经网络的学习过程）的需要而在每一层网络存储一些特定的参数，这一操作也提高了计算成本。</p>
<p>DeepMind 提出了 NFNet 以将标准化从等式中移除并提高训练表现。此外，该公司还提出了一项技术叫做</p>
<p><a href="https://arxiv.org/pdf/2102.06171.pdf">自适应梯度裁剪（adaptive gradient clipping）</a></p>
<p>这项技术可以对类似 ResNet 的神经网络使用更大的 batch size 进行训练，从而使网络的训练更加高效。在和 EfficientNet 保持相同精度的前提下，每单位计算资源（使用 GPU 的数量）下该方法能比前者减少 20-40% 的训练时间。</p>
<p><img src="https://res.infoq.com/news/2021/03/deepmind-NFNets-deep-learning/en/resources/6figure-2-1616684540852.jpg" alt="来源：不需要标准化的高性能的大规模图像识别 图4. ImageNet 验证精度 vs. 测试用的 GFLOP. 所有数字都对应单个模型和单个片段。即便是为了训练而被优化过，我们的 NFNet 模型在给定的 FLOP 预算前提下依然与大体量 EfficientNet 变体有一战之力。"></p>
<p><a href="https://github.com/deepmind/deepmind-research/tree/master/nfnets">源代码</a>已在谷歌 DeepMind 的 GitHub 仓库发布，基于名为 <a href="https://github.com/google/jax">JAX</a> 的新框架实现。如果想在 NFNet 上执行一步前向操作，只需要运行下面<a href="https://colab.research.google.com/github/deepmind/deepmind-research/blob/master/nfnets/nfnet_demo_colab.ipynb#scrollTo=qeotZfkBYrIg">这段代码</a>：</p>
<pre class="hljs"><code><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">inputs, is_training</span>):</span>
    model = nfnet.NFNet(num_classes=<span class="hljs-number">1000</span>,  variant=variant)
    <span class="hljs-keyword">return</span> model(inputs, is_training=is_training)[<span class="hljs-string">&#x27;logits&#x27;</span>]
net = hk.without_apply_rng(hk.transform(forward))
fwd = jax.jit(<span class="hljs-keyword">lambda</span> inputs: net.apply(params, inputs, is_training=<span class="hljs-literal">False</span>))
<span class="hljs-comment"># 我们将它分成两个 cell 以便我们不用重复地对 fwd fn 进行 jit 操作。</span>
logits = fwd(x[<span class="hljs-literal">None</span>]) <span class="hljs-comment"># 给 X 一个新的维度来让它的 batch size 变成 1。</span>
which_class = imagenet_classlist[<span class="hljs-built_in">int</span>(logits.argmax())]
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;ImageNet class: <span class="hljs-subst">{which_class}</span>.&#x27;</span>)
</code></pre>
<p>NFNet 也有一个 <a href="https://github.com/vballoli/nfnets-pytorch">Pytorch 实现版本</a>，这表明社区已经接纳了这个发行版：</p>
<pre class="hljs"><code><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn, optim
<span class="hljs-keyword">from</span> torchvision.models <span class="hljs-keyword">import</span> resnet18

<span class="hljs-keyword">from</span> nfnets <span class="hljs-keyword">import</span> WSConv2d
<span class="hljs-keyword">from</span> nfnets.agc <span class="hljs-keyword">import</span> AGC <span class="hljs-comment"># 需要测试</span>

conv = nn.Conv2d(<span class="hljs-number">3</span>,<span class="hljs-number">6</span>,<span class="hljs-number">3</span>)
w_conv = WSConv2d(<span class="hljs-number">3</span>,<span class="hljs-number">6</span>,<span class="hljs-number">3</span>)

optim = optim.SGD(conv.parameters(), <span class="hljs-number">1e-3</span>)
optim_agc = AGC(conv.parameters(), optim) <span class="hljs-comment"># 需要测试</span>

<span class="hljs-comment"># 应用 AGC 时，忽略模型的 fc。</span>
model = resnet18()
optim = torch.optim.SGD(model.parameters(), <span class="hljs-number">1e-3</span>)
optim = AGC(model.parameters(), optim, model=model, ignore_agc=[<span class="hljs-string">&#x27;fc&#x27;</span>])
</code></pre>
<p>最后，<a href="https://www.youtube.com/watch?v=rNkHjZtH0RQ">YouTube 上一个关于 NFNet 的视频</a>已经收获了超过 30,000 的播放量。</p>
<blockquote>
<p>如果发现译文存在错误或其他需要改进的地方，欢迎到 <a href="https://github.com/xitu/gold-miner">掘金翻译计划</a> 对译文进行修改并 PR，也可获得相应奖励积分。文章开头的 <strong>本文永久链接</strong> 即为本文在 GitHub 上的 MarkDown 链接。</p>
</blockquote>
<hr>
<blockquote>
<p><a href="https://github.com/xitu/gold-miner">掘金翻译计划</a> 是一个翻译优质互联网技术文章的社区，文章来源为 <a href="https://juejin.im">掘金</a> 上的英文分享文章。内容覆盖 <a href="https://github.com/xitu/gold-miner#android">Android</a>、<a href="https://github.com/xitu/gold-miner#ios">iOS</a>、<a href="https://github.com/xitu/gold-miner#%E5%89%8D%E7%AB%AF">前端</a>、<a href="https://github.com/xitu/gold-miner#%E5%90%8E%E7%AB%AF">后端</a>、<a href="https://github.com/xitu/gold-miner#%E5%8C%BA%E5%9D%97%E9%93%BE">区块链</a>、<a href="https://github.com/xitu/gold-miner#%E4%BA%A7%E5%93%81">产品</a>、<a href="https://github.com/xitu/gold-miner#%E8%AE%BE%E8%AE%A1">设计</a>、<a href="https://github.com/xitu/gold-miner#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD">人工智能</a>等领域，想要查看更多优质译文请持续关注 <a href="https://github.com/xitu/gold-miner">掘金翻译计划</a>、<a href="http://weibo.com/juejinfanyi">官方微博</a>、<a href="https://zhuanlan.zhihu.com/juejinfanyi">知乎专栏</a>。</p>
</blockquote>
</div>
<div class="footer"></div>
</body>
<script src="/assets/scripts/index.min.js"></script>
<script>init()</script>
</html>
